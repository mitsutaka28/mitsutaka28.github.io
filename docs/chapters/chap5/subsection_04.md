[メインページ](../../index.markdown)

[章目次](./chap5.md)
## 5.4. グラフプーリング

グラフフィルターは，グラフ構造を変えることなくノードの特徴を集約する．
フィルター後には各ノードは新しい特徴表現を持つ．
ノード表現を活用する，ノードに着目したタスクでは，グラフフィルター操作で十分な場合が多い．
しかし，グラフ全体に着目したタスクでは，グラフ全体の表現が必要である．
このような表現を得るためには，ノードからの情報を集約する必要がある．
グラフ表現を生成するために重要な情報は大きく分けて2種類ある．
1つはノードの特徴であり，もう1つはグラフ構造である．
グラフ表現はこの両者の情報を持っていることが望ましい．
古典的な畳込みニューラルネットワークと同様に，グラフプーリング層はグラフレベルでの表現を生成するために提案された．
初期に考案されたグラフプーリング層は概ね平坦なものだった． つまり,
1つのステップでノード表現から直接グラフレベルでの表現を生成する．
例えば，平均プーリング層や最大プーリング層は，各特徴ごとに導入されることで，グラフニューラルネットワークでも応用できる．
その後，元のグラフを段階的に荒くすることでグラフ情報を集約する，階層的グラフプーリングが考案された．
階層的グラフプーリングでは，図5.5のように複数のグラフプーリング層があることが多く,
各プーリング層ではいくつかのフィルターが積み重なっていることが多い．
通常,
1つのグラフプーリング層（平坦・階層的いずれについても）は，グラフを入力とし，粗視化したグラフを出力する．
式(5.2)でまとめたプロセスを思い出すと,

 $$ \mathbf{A}^{(\mathrm{op})}, \mathbf{F}^{(\mathrm{op})}=\operatorname{pool}\left\(\mathbf{A}^{(\mathrm{ip})}, \mathbf{F}^{(\mathrm{ip})}\right\)
    
\tag{5.41} $$ 

次に，まず平坦グラフプーリングを紹介し，次に階層的グラフプーリングを説明する．

### 平坦グラフプーリング

平坦グラフプーリングは，ノード表現から直接グラフレベルでの表現を生成する．
この層では新しいグラフは生成されず，ノードが一つ生成される．
したがって，平坦グラフプーリングでのプーリング操作は式(5.41)ではなく次のようにまとめることができる：

 

$$ \mathbf{f}\_G=\operatorname{pool}\left\(\mathbf{A}^{(\mathrm{ip})}, \mathbf{F}^{(\mathrm{ip})}\right\) \nonumber $$

 

ここで， $\mathbf{f}_{\mathcal{G}} \in \mathbb{R}^{1 \times d_{\text {op }}}$ はグラフ表現である．
次に代表的ないくつかの平坦プーリング層を紹介する．
古典的なCNNにおける最大プーリングと平均プーリングは，GNNについても適用することができる．
具体的に，グラフにおける最大プーリング操作は次のように書くことができる：

 

$$ \mathbf{f}\_G=\max \left\(\mathbf{F}^{(\mathrm{ip})}\right\) \nonumber $$

 

ここで，各チャンネルに対して次のように最大値を取得している：

 

$$ \mathbf{f}\_G[i]=\max \left\(\mathbf{F}_{:, i}^{(\mathrm{ip})}\right\) \nonumber $$

 

ここで， $\mathbf{F}_{:, i}^{(\mathrm{ip})}$ は $\mathbf{F}^{(\mathrm{ip})}$ の $i$ 番目のチャンネルである．
同様に，グラフにおける平均プーリング操作もチャンネルごとに適用することができる：

 

$$ \mathbf{f}\_G=a v e\left\(\mathbf{F}^{(\mathrm{ip})}\right\) \nonumber $$

 

(Li et al.,
2015)では，ゲート付きグローバルプーリングと呼ばれる，注意機構を用いた平坦プーリング操作が提案されている．
各ノードの重要度を測定する注意機構スコアを用いて，ノード表現を集約し，グラフ表現を生成する．
具体的には注意機構スコアは次のように計算される：

 

$$ s\_i=\frac{\exp \left\(h\left\(\mathbf{F}\_i^{(\mathrm{ip})}\right\)\right\)}{\sum_{v\_j \in \mathcal{V}} \exp \left\(h\left\(\mathbf{F}\_j^{(\mathrm{ip})}\right\)\right\)} \nonumber $$

 

ここで， $h$ は $\mathbf{F}^{(\mathrm{ip})}$ をスカラー値に変換する順伝搬ネットワークである．
その後ソフトマックス関数を用いて規格化される．
学習された注意機構スコアを用いて，ノード表現を集約し，次のようにグラフ表現が得られる：

 

$$ \mathbf{f}\_G=\sum_{v\_i \in \mathcal{V}} s\_i \cdot \tanh \left\(\mathbf{F}\_i^{(\mathrm{ip})} \boldsymbol{\Theta}_{i p}\right\) \nonumber $$

 

ここで， $\boldsymbol{\Theta}_{i p}$ は学習するパラメータであり，活性化関数 $\tanh()$ は恒等関数でもよい．

平坦グラフプーリング操作はフィルター層に組み込まれているものもある．
例えば, (Li et al.,
2015)では，すべてのノードとつながっている「偽」ノードがグラフに追加されている．
この「偽」ノードの表現はフィルター操作の過程で学習することができる．
この表現は，グラフ内のすべてのノードに接続されているので，グラフ全体の情報を捉えることができる．
したがって，このような「偽」ノードの表現をグラフ表現として活用することもできる．

### 階層的グラフプーリング

平坦プーリング層は，ノード表現を集約してグラフ表現を生成する際，通常階層的なグラフ構造を無視することが多い．
階層的グラフプーリング層では，グラフ表現の生成に至るまで段階的にグラフを粗くすることによって，階層的グラフ構造を取り入れる．
どのようにグラフを粗くしていくかによって，階層的グラフプーリングを大別することができる．
そのうちの1つは，サブサンプリングによってグラフを粗くする．
つまり，重要なノードを選び粗視化したグラフとする．
別のタイプの階層的グラフプーリングでは，入力グラフのノードを統合して「スーパーノード」を構成し，それらを用いて粗視化したグラフを生成する．
この2種類の階層的グラフプーリングの主な違いは，サブサンプリング型では元のグラフのノードを保持するのに対し，スーパーノード型では粗視化グラフ用に新しくノードを生成するという点である．
ここからは，各種類で代表的な技術を紹介する．
具体的には，式(5.41)の階層的グラフプーリング層の処理について，どのようにして粗視化グラフ $\mathbf{A}^{(\mathrm{op})}$ とノードの特徴 $\mathbf{F}^{(\mathrm{op})}$ を生成するのかを説明する．

#### ダウンサンプリング型プーリング

入力グラフを粗視化するため，ある重要度に応じて $N_{\mathrm{op}}$ 個のノードを選び，これらのノードについてのグラフ構造やノードの特徴を用いて粗視化グラフとする．
ダウンサンプリング型プーリングには3つの重要な要素がある．
1つ目は，ダウンサンプリングの基準とする量を計算することである．
2つ目は，粗視化グラフのためにグラフ構造を生成することである．
そして3つ目は，粗視化グラフのノードの特徴を生成することである．
これら3つの要素の設計が各ダウンサンプリング型手法によって異なることが多い．
次に代表的なダウンサンプリング型グラフプーリングについて説明する．

(Gao and Ji,
2019)で提案されたgPoolという方法は，グラフの粗視化に初めて導入されたダウンサンプリング手法である．
gPoolではノードの重要度は，入力ノードの特徴 $\mathbf{F}^{(\mathrm{ip})}$ から次のように学習される．

 $$ \mathbf{y}=\frac{\mathbf{F}^{(\mathrm{ip})} \mathbf{p}}{\\|\mathbf{p}\\|}
    
\tag{5.42} $$ 

ここで， $\mathbf{F}^{(\mathrm{ip})} \in \mathbb{R}^{N_{\mathrm{ip}} \times d_{\mathrm{ip}}}$ は入力ノードの特徴を表す行列であり， $\mathbf{p} \in \mathbb{R}^{d_{\mathrm{ip}}}$ は学習されるベクトルである．
このべクトルは入力の特徴を重要度スコアに射影する．
重要度スコア $\mathbf{y}$ を計算したあとには，ノードをそれに基づいてランク付けし，重要度の高い $N_{\mathrm{op}}$ 個のノードを選び出す．

 

$$ \mathrm{idx}=\operatorname{rank}\left\(\mathbf{y}, N_{\mathrm{op}}\right\) \nonumber $$

 

ここで， $N_{\mathrm{op}}$ は粗視化グラフのノード数で，idxはその選ばれた上位 $N_{\mathrm{op}}$ 個のノードのインデックスである．
idxでラベル付けされたこれら上位ノードについて，次にグラフ構造をノードの特徴を生成する．
具体的には，粗視化グラフのグラフ構造はもとの入力グラフ構造から次のように生成することができる:

 

$$ \mathbf{A}^{(\mathrm{op})}=\mathbf{A}^{(\mathrm{ip})}(\mathrm{idx}, \mathrm{idx}) \nonumber $$

 

ここで， $\mathbf{A}^{(\mathrm{ip})}(\mathrm{idx}, \mathrm{idx})$ は $\mathbf{A}$ からidxに対応する行と列を抜き出す操作を表す．
同様に，ノードの特徴も入力ノードの特徴から抜き出すことができる． (Gao
and Ji,
2019)では，入力の特徴から新しい特徴への情報の流れを制御するためにゲート構造が導入されている．
具体的には，重要度の高いノードがより多くの情報を粗視化をグラフに流せるようになっており，次のようにモデル化することができる：
 

$$
\begin{aligned}
    \tilde{\mathbf{y}} &=\sigma(\mathbf{y}(\mathrm{idx})) \nonumber \\ 
    \tilde{\mathbf{F}} &=\mathbf{F}^{(\mathrm{ip})}(\mathrm{idx},:) \nonumber \\ 
    \mathbf{F}\_p &=\tilde{\mathbf{F}} \odot\left\(\tilde{\mathbf{y}}_{d_{\mathrm{ip}}}^{\top}\right\) \nonumber
\end{aligned}
$$
 
ここで， $\sigma()$ はシグモイド関数であり，重要度スコアを(0,1)に変換する．
 $\mathbf{1}_{d_{\mathrm{ip}}} \in \mathbb{R}^{d_{\mathrm{ip}}}$ はすべての要素が1のベクトルである．
 $\mathbf{y}(\mathrm{idx})$ は $\mathbf{y}$ からidxに対応する要素を抜き出し， $\mathbf{F}^{(\mathrm{ip})}(\mathrm{idx})$ はidxに対応する行を取得する．

gPoolでは，式(5.42)のように，入力の特徴のみに基づいて重要度スコアが学習される．
この段階でグラフ構造を組み込むため, (Lee et al.,
2019)ではGCNフィルタが用いられた．
具体的には，重要度スコアが次のように計算することができる．

 

$$ \mathbf{y}=\alpha\left\(\mathrm{GCN}-\mathrm{Filter}\left\(\mathbf{A}^{(\mathrm{ip})}, \mathbf{F}^{(\mathrm{ip})}\right\)\right\) $$

 

ここで， $\alpha$ は $\tanh$ などの活性化関数である．
なお， $\mathbf{y}$ は行列ではなくベクトルである．
つまり，GCNフィルターの出力チャンネル数を1にしているということである．
このグラフプーリング操作はSAGPoolと呼ばれる．

#### スーパーノード型プーリング

ダウンサンプリング型の階層的グラフプーリングは，ある重要度に基づいてノードを選択し，入力グラフを粗視化する方法だった．
その過程で，選ばれなかったノードは捨てられるのでそれらの情報は失われてしまう．
スーパーノード型プーリングは，スーパーノードを生成することによって入力グラフの粗視化を試みる．
具体的には，入力グラフのノードを異なるクラスターに割り当てる方法を学習し，そのクラスターをスーパーノードとして取り扱う．
このスーパーノードが粗視化グラフにおけるノードとして扱われることになる．
スーパーノード間のエッジとスーパーノードの特徴を生成することで，粗視化グラフを構成する．
スーパーノード型のグラフプーリングには3つの重要な要素がある．
1つ目は，スーパーノードを粗視化グラフのノードとして生成すること,
2つ目は，粗視化グラフのグラフ構造を生成すること,
そして3つ目は，粗視化グラフのノードの特徴を生成することである．
次に，スーパーノード型プーリングの代表的ないくつかの方法を紹介する．

***diffpool***

diffpoolアルゴリズムは，微分可能な方法でスーパーノードを生成する．
具体的には，GCNフィルターを用いて入力グラフのノードからスーパーノードへ割り当てる行列を学習する．

 $$ \mathbf{S}=\operatorname{softmax}\left\(\mathrm{GCN}-\mathrm{Filter}\left\(\mathbf{A}^{(\mathrm{ip})}, \mathbf{F}^{(\mathrm{ip})}\right\)\right\)
    
\tag{5.44} $$ 

ここで， $\mathbf{S} \in \mathbb{R}^{N_{\mathrm{ip}} \times N_{\mathrm{op}}}$ は学習される行列である．
式(5.3)で示したように， $\mathbf{F}^{(\mathrm{ip})}$ は直近のグラフフィルター層の出力である．
しかし, (Ying et al.,
2018c)ではプーリング層の入力はその前のプーリング層の出力（つまり，学習ブロック $\mathbf{F}^{(\mathrm{ib})}$ の入力,
5.2.2節のブロック構造参照)だった．
さらに，式(5.44)では1つのフィルターしかないが，いくつかのGCNフィルターを積み重ねて行列を学習させることもできる．
学習される割り当て行列の各列をスーパーノードとみなすことができる．
そして，ソフトマックス関数を行ごとに適用する．
したがって，各行は合計が1に規格化されていることになる．
 $i$ 行目の $j$ 列めの要素は， $i$ 番目のノードが $j$ 番目のノードに割り当てられる可能性を表している．
次に，この割り当て行列 $\mathbf{S}$ を用いて，粗視化グラフのグラフ構造とノードの特徴を生成する．
具体的には，この割り当て行列 $\mathbf{S}$ を活用することで，粗視化グラフのグラフ構造は次のように生成することができる:

 

$$ \mathbf{A}^{(\mathrm{op})}=\mathbf{S}^{\top} \mathbf{A}^{(\mathrm{ip})} \mathbf{S} \in \mathbb{R}^{N_{\mathrm{op}} \times N_{\mathrm{op}}}\nonumber $$

 

同様に，スーパーノードの特徴は， $\mathbf{S}$ を用いて入力グラフの線型結合によって計算することができる:

 

$$ \mathbf{F}^{(\mathrm{op})}=\mathbf{S}^{\top} \mathbf{F}^{(\text {inter) }} \in \mathbb{R}^{N_{\mathrm{op}} \times d_{\mathrm{op}}} \nonumber $$

 

ここで， $\mathbf{F}^{(\text {inter })} \in \mathbb{R}^{N_{\mathrm{ip}} \times d_{\mathrm{op}}}$ はGCNフィルターによる学習によって得られる，途中の特徴量である:

 $$ \mathbf{F}^{(\text {inter })}=\mathrm{GCN}-\mathrm{Filter}\left\(\mathbf{A}^{(\mathrm{ip})}, \mathbf{F}^{(\mathrm{ip})}\right\)
    
\tag{5.45} $$ 

式(5.45)では1つのフィルターのみだが，複数のGCNフィルターを積み重ねることもできる．
diffpoolは次のようにまとめることができる:

 

$$ \mathbf{A}^{(\mathrm{op})}, \mathbf{F}^{(\mathrm{op})}=\operatorname{diffpool}\left\(\mathbf{A}^{(\mathrm{ip})}, \mathbf{F}^{(\mathrm{ip})}\right\) \nonumber $$

 

***EigenPooling***

EigenPooling(Ma et al.,
2019b)はスペクトルクラスタリングを用いてスーパーノードを生成するアルゴリズムで，粗視化グラフのグラフ構造とノードの特徴に重点を置いている．
スペクトルクラスタリングを適用後には重複のないクラスターが得られる．
このクラスターを粗視化グラフのスーパーノードとみなすことができる．
入力グラフのノードとスーパーノードの対応を表す行列は $\mathbf{S} \in\{0,1\}^{N_{\mathrm{ip}} \times N_{\text {op }}}$ と書くことができる．
ここで，各行について1つの要素だけが1であり，その他の要素は0になる．
より具体的には， $i$ 番目のノードが $j$ 番目のスーパーノードに割り当てられているときのみ， $S_{i, j}=1$ となる．
 $k$ 番目のスーパーノードについて，対応するクラスターのグラフ構造を $\mathbf{A}^{(k)} \in \mathbb{R}^{N^{(k)} \times N^{(k)}}$ と書く．
ここで， $N^{(k)}$ はこのクラスターのノードの数である．
次にサンプリング演算子 $\mathbf{C}^{(k)} \in\{0,1\}^{N_{\mathrm{ip}} \times N^{(k)}}$ を次のように定義する:

 

$$ \mathbf{C}_{i, j}^{(k)}=1 \quad \text { if and only if } \quad \Gamma^{(k)}(j)=v\_i \nonumber $$

 

ここで， $\Gamma^{(k)}$ は $k$ 番目のクラスターのノード一覧を表し， $\Gamma^{(k)}(j)=v_i$ はノード $v_i$ がこのクラスターの $j$ 番目のノードに対応していることを表す．
このサンプリング演算子を用いて， $k$ 番目のクラスターの隣接行列は次のように定義することができる:

 

$$ \mathbf{A}^{(k)}=\left\(\mathbf{C}^{(k)}\right\)^{\top} \mathbf{A}^{(\mathrm{ip})} \mathbf{C}^{(k)} \nonumber $$

 

次に，粗視化グラフのグラフ構造とノードの特徴を生成するプロセスについて説明する．
スーパーノード間のグラフ構造を構成するため，元のグラフのクラスター間のつながりのみを考慮する．
そのた目に，まず初めに入力グラフのクラスター内隣接行列を生成する．
この行列は各クラスター内のエッジのみから構成され，次のように書くことができる:

 

$$ \mathbf{A}_{i n t}=\sum_{k=1}^{N_{\mathrm{op}}} \mathbf{C}^{(k)} \mathbf{A}^{(k)}\left\(\mathbf{C}^{(k)}\right\)^{\top}\nonumber $$

 

そして，クラスター同士のエッジのみから構成される，クラスター間隣接行列は
 $\mathbf{A}_{e x t}=\mathbf{A}-\mathbf{A}_{i n t}$ と書くことができる．
粗視化グラフの隣接行列は次のように得られる:

 

$$ \mathbf{A}^{\mathrm{op}}=\mathbf{S}^{\top} \mathbf{A}_{e x t} \mathbf{S}\nonumber $$

 

ノードの特徴を生成するためにはグラフフーリエ変換を用いる．
具体的には，各サブグラフ（ここではクラスター）のグラフ構造とノードの特徴を用いて，対応するスーパーノードのノードの特徴を生成する．
まず， $k$ 番目のクラスターに注目してこのプロセスを説明する．
 $\mathbf{L}^{(k)}$ をこのサブグラフのラプラシアン行列とし,
 $\mathbf{u}\_1^{(k)}, \ldots, \mathbf{u}_{n^{(k)}}^{(k)}$ を対応する固有ベクトルとする．
このサブグラフのノードの特徴はサンプリング演算子 $\mathbf{C}^{(k)}$ を用いることで $\mathbf{F}^{(\mathrm{ip})}$ から次のように抜き出すことができる．

 

$$ \mathbf{F}_{\mathrm{ip}}^{(k)}=\left\(\mathbf{C}^{(k)}\right\)^{\top} \mathbf{F}^{(\mathrm{ip})} \nonumber $$

 

ここで， $\mathbf{F}_{\mathrm{ip}}^{(k)} \in \mathbb{R}^{N^{(k)} \times d_{\mathrm{ip}}}$ は $k$ 番目のクラスターのノードの入力特徴量である．
次に，グラフフーリエ変換を適用することで， $\mathbf{F}_{\mathrm{ip}}^{(k)}$ のすべてのチャンネルについてグラフフーリエ係数を得る:

 

$$ \mathbf{f}\_i^{(k)}=\left\(\mathbf{u}\_i^{(k)}\right\)^{\top} \mathbf{F}_{\mathrm{ip}}^{(k)} \quad \text { for } \quad i=1, \ldots, N^{(k)} \nonumber $$

 

ここで， $\mathbf{f}\_i^{(k)} \in \mathbb{R}^{1 \times d_{\mathrm{ip}}}$ はすべての特徴量チャンネルについての $i$ 番目のグラフフーリエ係数からなる．
 $k$ 番目のスーパーノードのノードの特徴は，これらの係数を結合することで得られる:

 

$$ \mathbf{f}^{(k)}=\left\[\mathbf{f}\_1^{(k)}, \ldots, \mathbf{f}_{N^{(k)}}^{(k)}\right\] \nonumber $$

 

スーパーノードの特徴を生成するために，最初の数個の係数のみを利用する場合が多いが，これには2つの理由がある．
第一に，異なるサブグラフのノード数はそれぞれ異なるため，同じ次元の特徴を確保するためには，いくつかの係数を捨てる必要がある．
第二に，現実にはグラフ信号の大部分は滑らかであるため，最初のいくつか係数は重要な情報のほとんどを捉えている場合が多い．


[メインページ](../../index.markdown)

[章目次](./chap5.md)

[前の節へ](./subsection_03.md) [次の節へ](./subsection_05.md)


