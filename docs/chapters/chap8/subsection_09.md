[メインページ](../../index.markdown)

[章目次](./chap8.md)
## 8.9. 参考文献

複雑グラフに対する代表的なグラフニューラルネットワークを紹介してきたが，この領域は常に新たな研究が実施されている．

-   Zhang *et al*.(2019a)では，ヘテログラフに対するグラフニューラルネットワークの設計に，ヘテロ近傍(heterogeneous neighbors)によるランダムウォークを使ったサンプリングを導入している．

-   Sankar *et al*.(2018)では，離散型ダイナミックグラフに対してセルフアテンション機構が利用されている．

-   Bai *et al*.(2019)では，ハイパーグラフニューラルネットワークのモデリングにアテンション機構を導入している．

-   Jiang *et al*.(2019)とMa *et al*.(2020b)では、ダイナミックグラフに対するグラフニューラルネットワークが設計されている．


    Chanらはこの拡散過程を一般化し，ノードの情報量（ここでは $\symbf{h}=\symbf{D}^{-1}\symbf{\varphi}$ と表現される． $D$ は次数行列）とエッジの重みに依存する形に拡張している．具体的には，次数の逆数によってスケーリングした情報量 $\symbf{h}=\symbf{D}^{-1}\symbf{\varphi}$ を導入し，エッジの重み $w_{uv}$ に基づいて， $\symbf{h}$ が高いノードから低いノードへと情報が流れる様子を示す微分方程式を提案している．この微分方程式は次のようになる： 

$$
 \dfrac{d\varphi_u}{dt} = \sum_{u,v\in \symscr{E}}w_{uv}(\symbf{h}_v-\symbf{h}_u) $$


 ここで， $\symscr{E}$ はエッジ集合である．この一般化した微分方程式は，ラプラシアン行列 $\symbf{L}$ を用いた元の拡散過程 $\dfrac{d\symbf{\varphi}}{dt} = -\symbf{L}\symbf{\varphi}$ を拡張し，エッジの重みとノードの情報量 $\symbf{h}$ を考慮した情報の流れをより正確に表現している．Chanらはこのアイディアを「ハイパーグラフのラプラシアン行列の定義」に利用している．

    Yadati *et al*.(2019)は，上述の議論を基にハイパーエッジから一対一関係を抽出する方法を確立し，グラフフィルタの設計を行っている．




$$
 \tilde{\symbf{A}}^{(l-1)} = \tilde{\symbf{\symbf{D}}}^{-\tfrac{1}{2}}(\symbf{A}^{(l-1)} + \symbf{I})\tilde{\symbf{\symbf{D}}}^{-\tfrac{1}{2}} $$


 ここで， $\tilde{\symbf{D}}\_{ii}=\sum_j(\symbf{A}^{(l-1)}+\symbf{I})_{i,j}$ である．

[メインページ](../../index.markdown)

[章目次](./chap8.md)

[前の節へ](./subsection_08.md) [次の節へ](./subsection_10.md)

[^1]: 進んだ訳注：Chan *et al*.(2018)では，グラフ上での拡散過程（ランダムウォーク）とラプラシアン行列の関係性を調査している．そこでは，各ノードで定義される量を $\symbf{\varphi}$ としたとき，微分方程式 $d\symbf{\varphi}/dt = -\symbf{L}\symbf{\varphi}$  は「ラプラシアン行列 $\symbf{L}$ を通じて，ノードの情報がグラフ上の各ノード間にどのように拡散するか」を表現するものであると述べている．
[^2]: 訳注：第5章で扱ったように，renormalization trickによる変換を行っている： 
